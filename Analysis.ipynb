{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:97% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:97% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) #no scientific notation\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"Geo Modified Dataset - 80 cells - width==0.1 .csv\", index_col=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_set = {'ASIAN', 'BLACK', 'HISPANIC', 'NATIVE AMERICAN', 'OTHER', 'WHITE'}\n",
    "\n",
    "no_NA_race_set = {'ASIAN', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE'}\n",
    "\n",
    "for race1 in no_NA_race_set:\n",
    "    \n",
    "    for race2 in no_NA_race_set:\n",
    "        df[f\"INTERACTION: D_{race1} X {race2} Racial Composition\"] = df[f'{race1} - (D_Race)'] * df[f'GEO: {race2} Racial Composition']\n",
    "\n",
    "        df[f\"INTERACTION: D_{race1} X {race2} Percent of Charges that were CHANGED\"] = df[f'{race1} - (D_Race)'] * df[f'GEO: {race2} Percent of Charges that were CHANGED']\n",
    "\n",
    "        df[f\"INTERACTION: D_{race1} X {race2} Average Speed NOT in 9,14 MPH\"] = df[f'{race1} - (D_Race)'] * df[f'GEO: {race2} Average Speed NOT in 9,14 MPH']\n",
    "\n",
    "        df[f\"INTERACTION: D_{race1} X D_Male\"] = df[f'{race1} - (D_Race)'] * df[\"Male\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Chi-Squared Test for Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# race_set = {'ASIAN', 'BLACK', 'HISPANIC', 'NATIVE AMERICAN', 'OTHER', 'WHITE'}\n",
    "speeding_bool_set = {\"Speed Altered\", 'Speed NOT Altered'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "altered = []\n",
    "not_altered = []\n",
    "\n",
    "\n",
    "for x in df['Speed Over Posted Limit']:\n",
    "    if x==9:\n",
    "        altered.append(1)\n",
    "        not_altered.append(0)\n",
    "    elif 10 <= x <= 14:\n",
    "        altered.append(0)\n",
    "        not_altered.append(1)\n",
    "    else:\n",
    "        altered.append(np.nan)\n",
    "        not_altered.append(np.nan)\n",
    "\n",
    "df['Speed Altered'] = altered\n",
    "df['Speed NOT Altered'] = not_altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contingency_table = pd.DataFrame({x:[0 for race in sorted(list(race_set))] for x in speeding_bool_set}, index=sorted(list(race_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed Altered</th>\n",
       "      <th>Speed NOT Altered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASIAN</th>\n",
       "      <td>3626</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLACK</th>\n",
       "      <td>10670</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HISPANIC</th>\n",
       "      <td>7039</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATIVE AMERICAN</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTHER</th>\n",
       "      <td>3179</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHITE</th>\n",
       "      <td>24843</td>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Speed Altered  Speed NOT Altered\n",
       "ASIAN                     3626                236\n",
       "BLACK                    10670               1049\n",
       "HISPANIC                  7039                726\n",
       "NATIVE AMERICAN             36                 10\n",
       "OTHER                     3179                267\n",
       "WHITE                    24843               1479"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in contingency_table:\n",
    "    for ind in contingency_table.index:\n",
    "        temp_col = []\n",
    "        for x in zip(df[col], df[f\"{ind} - (D_Race)\"]):\n",
    "            temp_col.append(x[0]==x[1]==1)\n",
    "            \n",
    "        contingency_table[col].loc[ind] = sum(temp_col)\n",
    "\n",
    "        \n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Chi2 Test\n",
    "\n",
    "Returns (in order):\n",
    "\n",
    "- The test statistic.\n",
    "\n",
    "- The p-value of the test\n",
    "\n",
    "- Degrees of freedom\n",
    "\n",
    "- The expected frequencies, based on the marginal sums of the table.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test statistic == 231.28605043787022\n",
      "\n",
      " P-Value == 5.66938794054964e-48\n",
      "\n",
      " Degrees of Freedom == 5\n",
      "\n",
      " Expected Frequencies == \n",
      " [[ 3588.33269375   273.66730625]\n",
      " [10888.57349511   830.42650489]\n",
      " [ 7214.76006396   550.23993604]\n",
      " [   42.7403687      3.2596313 ]\n",
      " [ 3201.81109857   244.18890143]\n",
      " [24456.78227991  1865.21772009]]\n",
      "\n",
      " Difference between Actual and Expected Frequencies (Actual - Expected) == \n",
      "\n",
      "                  Speed Altered  Speed NOT Altered\n",
      "ASIAN                37.667306         -37.667306\n",
      "BLACK              -218.573495         218.573495\n",
      "HISPANIC           -175.760064         175.760064\n",
      "NATIVE AMERICAN      -6.740369           6.740369\n",
      "OTHER               -22.811099          22.811099\n",
      "WHITE               386.217720        -386.217720\n"
     ]
    }
   ],
   "source": [
    "chi2_result = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\n Test statistic == {chi2_result[0]}\")\n",
    "print(f\"\\n P-Value == {chi2_result[1]}\")\n",
    "print(f\"\\n Degrees of Freedom == {chi2_result[2]}\")\n",
    "print(f\"\\n Expected Frequencies == \\n {chi2_result[3]}\")\n",
    "\n",
    "print(f\"\\n Difference between Actual and Expected Frequencies (Actual - Expected) == \\n\\n {contingency_table-chi2_result[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Maybe do this in R - Python implementation is not great (scipy doesn't have a regression summary / p-value; statsmodel doesn't make sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "regression_df = df[(9<=df['Speed Over Posted Limit']) & (df['Speed Over Posted Limit']<=14)] # all observations where 9 <= speed <= 14 \n",
    "\n",
    "#replace np.nan with 0\n",
    "for x in list(zip(regression_df.isnull().sum(), regression_df.columns)):\n",
    "    if x[0]!=0:\n",
    "        regression_df[x[1]] = regression_df[x[1]].fillna(value=0)\n",
    "    \n",
    "y = regression_df['Speed Altered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete non-boolean or non-float columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in regression_df.columns:\n",
    "    if regression_df[col].dtype not in ('bool', 'float64', 'int64'):\n",
    "        if regression_df[col].dtype=='object':\n",
    "            del regression_df[col]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Multicollinearity-Causing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinear_cols = ['GEO: OTHER Racial Composition',\n",
    "'OTHER - (D_Race)',\n",
    "'Headquarters and Special Operations - (D_SubAgency)',\n",
    "'ESERO - (D_Violation Type)',\n",
    "'Number of writeups'\n",
    "                      ]\n",
    "\n",
    "for col in multicollinear_cols:\n",
    "    try:\n",
    "        del regression_df[col]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {col}, exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "misc_del_list = set(\n",
    "['Speed Altered',\n",
    "'Speed NOT Altered',\n",
    "    \n",
    "'Citation - (D_Violation Type)',\n",
    "'Warning - (D_Violation Type)',\n",
    "            \n",
    "'Citation - (D_Search Outcome)',\n",
    "'Warning - (D_Search Outcome)',            \n",
    "])\n",
    "\n",
    "for col in regression_df:\n",
    "    if 'D_Search Outcome' in col:\n",
    "        misc_del_list.add(col)\n",
    "\n",
    "for col in misc_del_list:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        del regression_df[col]\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"failed to delete {col}, exception: {e}\")\n",
    "\n",
    "X = regression_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1) #keep this random_state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that there's an even proportion of True, False in training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(y_test)-mean(y_train) == 0.00018811136192620204\n",
      "Discrepancy in raw count == 9.9999999999969\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean(y_test)-mean(y_train) == {(abs(np.mean(y_test)-np.mean(y_train)))}\")\n",
    "\n",
    "print(f'Discrepancy in raw count == { (abs(np.mean(y_test)-np.mean(y_train)))*len(regression_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- Do I need to normalize the data before L1?\n",
    "- Should we add interaction terms for the geo vars?\n",
    "\n",
    "Thoughts:\n",
    "\n",
    "- Results with/without L1 are very different\n",
    "  * All Geo vars have the same coefficients in normal logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With an Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.width = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "l1_model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "l1_model.fit(X, y)\n",
    "l1_coefficients = l1_model.coef_.tolist()[0]\n",
    "zipped_l1_coefs = list(zip([['Intercept']+list(regression_df.columns)][0], [l1_model.intercept_.tolist()[0]]+[round(x,3) for x in l1_coefficients]))\n",
    "\n",
    "\n",
    "\n",
    "log_model = LogisticRegression(max_iter=8000) #default max_iter==100\n",
    "log_model.fit(X, y)\n",
    "log_coefficients = log_model.coef_.tolist()[0]\n",
    "zipped_log_coefs = list(zip(['Intercept']+[list(regression_df.columns)][0], [log_model.intercept_.tolist()[0]]+[round(x,3) for x in log_coefficients]))\n",
    "\n",
    "# OLS_model = LinearRegression()\n",
    "# OLS_model.fit(X, y)\n",
    "# OLS_coefficients = log_model.coef_.tolist()[0]\n",
    "# zipped_OLS_coefs = list(zip(['Intercept']+[list(regression_df.columns)][0], [OLS_model.intercept_.tolist()[0]]+[round(x,3) for x in OLS_coefficients]))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({'Variable': [x[0] for x in zipped_l1_coefs],\n",
    "                           'L1 Coefficient': [x[1] for x in zipped_l1_coefs],\n",
    "                           'Normal Logit Coefficient': [x[1] for x in zipped_log_coefs],\n",
    "#                            'OLS Coefficient': [x[1] for x in zipped_OLS_coefs]\n",
    "                          })\n",
    "\n",
    "results_df = results_df.sort_values(by=['L1 Coefficient'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Variable  L1 Coefficient  Normal Logit Coefficient\n",
      "0                                            Intercept       23.451998                 22.628433\n",
      "21                                    WHITE - (D_Race)        4.403000                  7.991000\n",
      "18                                    BLACK - (D_Race)        4.090000                  5.584000\n",
      "17                                    ASIAN - (D_Race)        4.084000                  2.497000\n",
      "19                                 HISPANIC - (D_Race)        4.013000                  3.834000\n",
      "20                          NATIVE AMERICAN - (D_Race)        2.888000                  0.052000\n",
      "112                      INTERACTION: D_OTHER X D_Male        1.609000                  0.851000\n",
      "22             1st District, Rockville - (D_SubAgency)        1.156000                  3.842000\n",
      "25               4th District, Wheaton - (D_SubAgency)        1.129000                  3.664000\n",
      "8                                                Belts        0.942000                  1.855000\n",
      "26            5th District, Germantown - (D_SubAgency)        0.911000                  3.914000\n",
      "23              2nd District, Bethesda - (D_SubAgency)        0.877000                  3.262000\n",
      "27   6th District, Gaithersburg / Montgomery Villag...        0.831000                  3.096000\n",
      "24         3rd District, Silver Spring - (D_SubAgency)        0.258000                  3.966000\n",
      "75   GEO: NATIVE AMERICAN Percent of Charges that w...        0.163000                  0.589000\n",
      "133  INTERACTION: D_WHITE X OTHER Percent of Charge...        0.082000                  0.315000\n",
      "2                                          Speed Limit        0.059000                  1.078000\n",
      "5                                             Latitude        0.041000                  0.587000\n",
      "44     GEO: HISPANIC Number of Citations for Each Stop        0.036000                  0.152000\n",
      "115  INTERACTION: D_OTHER X HISPANIC Average Speed ...        0.033000                 -0.080000\n",
      "124  INTERACTION: D_OTHER X ASIAN Average Speed NOT...        0.025000                 -0.080000\n",
      "92   INTERACTION: D_BLACK X ASIAN Average Speed NOT...        0.023000                 -0.137000\n",
      "111  INTERACTION: D_OTHER X BLACK Average Speed NOT...        0.018000                 -0.080000\n",
      "118  INTERACTION: D_OTHER X OTHER Average Speed NOT...        0.016000                 -0.080000\n",
      "121  INTERACTION: D_OTHER X WHITE Average Speed NOT...        0.012000                 -0.080000\n",
      "140  INTERACTION: D_WHITE X ASIAN Average Speed NOT...        0.010000                 -0.165000\n",
      "105  INTERACTION: D_HISPANIC X WHITE Average Speed ...        0.010000                 -0.119000\n",
      "37            GEO: BLACK Average Speed NOT in 9,14 MPH        0.009000                  0.037000\n",
      "43         GEO: HISPANIC Average Speed NOT in 9,14 MPH        0.009000                  0.037000\n",
      "60            GEO: WHITE Average Speed NOT in 9,14 MPH        0.008000                  0.037000\n",
      "49            GEO: OTHER Average Speed NOT in 9,14 MPH        0.004000                  0.037000\n",
      "102  INTERACTION: D_HISPANIC X OTHER Average Speed ...        0.004000                 -0.119000\n",
      "150  INTERACTION: D_ASIAN X OTHER Average Speed NOT...        0.001000                 -0.108000\n",
      "100  INTERACTION: D_HISPANIC X OTHER Racial Composi...        0.000000                 -0.022000\n",
      "110  INTERACTION: D_OTHER X BLACK Percent of Charge...        0.000000                  0.075000\n",
      "113  INTERACTION: D_OTHER X HISPANIC Racial Composi...        0.000000                 -0.008000\n",
      "77     INTERACTION: D_BLACK X BLACK Racial Composition        0.000000                 -0.054000\n",
      "81   INTERACTION: D_BLACK X HISPANIC Racial Composi...        0.000000                 -0.054000\n",
      "82   INTERACTION: D_BLACK X HISPANIC Percent of Cha...        0.000000                  0.066000\n",
      "84     INTERACTION: D_BLACK X OTHER Racial Composition        0.000000                 -0.054000\n",
      "106  INTERACTION: D_HISPANIC X ASIAN Racial Composi...        0.000000                 -0.022000\n",
      "109    INTERACTION: D_OTHER X BLACK Racial Composition        0.000000                 -0.008000\n",
      "87     INTERACTION: D_BLACK X WHITE Racial Composition        0.000000                 -0.054000\n",
      "90     INTERACTION: D_BLACK X ASIAN Racial Composition        0.000000                 -0.054000\n",
      "93   INTERACTION: D_HISPANIC X BLACK Racial Composi...        0.000000                 -0.022000\n",
      "103  INTERACTION: D_HISPANIC X WHITE Racial Composi...        0.000000                 -0.022000\n",
      "97   INTERACTION: D_HISPANIC X HISPANIC Racial Comp...        0.000000                 -0.022000\n",
      "101  INTERACTION: D_HISPANIC X OTHER Percent of Cha...        0.000000                  0.029000\n",
      "142  INTERACTION: D_ASIAN X BLACK Percent of Charge...        0.000000                  0.040000\n",
      "114  INTERACTION: D_OTHER X HISPANIC Percent of Cha...        0.000000                  0.075000\n",
      "139  INTERACTION: D_WHITE X ASIAN Percent of Charge...        0.000000                  0.315000\n",
      "144                      INTERACTION: D_ASIAN X D_Male        0.000000                  0.409000\n",
      "145  INTERACTION: D_ASIAN X HISPANIC Racial Composi...        0.000000                  0.000000\n",
      "146  INTERACTION: D_ASIAN X HISPANIC Percent of Cha...        0.000000                  0.040000\n",
      "148    INTERACTION: D_ASIAN X OTHER Racial Composition        0.000000                  0.000000\n",
      "149  INTERACTION: D_ASIAN X OTHER Percent of Charge...        0.000000                  0.040000\n",
      "151    INTERACTION: D_ASIAN X WHITE Racial Composition        0.000000                  0.000000\n",
      "152  INTERACTION: D_ASIAN X WHITE Percent of Charge...        0.000000                  0.040000\n",
      "154    INTERACTION: D_ASIAN X ASIAN Racial Composition        0.000000                  0.000000\n",
      "155  INTERACTION: D_ASIAN X ASIAN Percent of Charge...        0.000000                  0.040000\n",
      "138    INTERACTION: D_WHITE X ASIAN Racial Composition        0.000000                  0.105000\n",
      "116    INTERACTION: D_OTHER X OTHER Racial Composition        0.000000                 -0.008000\n",
      "135    INTERACTION: D_WHITE X WHITE Racial Composition        0.000000                  0.105000\n",
      "129  INTERACTION: D_WHITE X HISPANIC Racial Composi...        0.000000                  0.105000\n",
      "125    INTERACTION: D_WHITE X BLACK Racial Composition        0.000000                  0.105000\n",
      "74             GEO: NATIVE AMERICAN Racial Composition        0.000000                  0.021000\n",
      "122    INTERACTION: D_OTHER X ASIAN Racial Composition        0.000000                 -0.008000\n",
      "120  INTERACTION: D_OTHER X WHITE Percent of Charge...        0.000000                  0.075000\n",
      "119    INTERACTION: D_OTHER X WHITE Racial Composition        0.000000                 -0.008000\n",
      "141    INTERACTION: D_ASIAN X BLACK Racial Composition        0.000000                  0.000000\n",
      "117  INTERACTION: D_OTHER X OTHER Percent of Charge...        0.000000                  0.075000\n",
      "123  INTERACTION: D_OTHER X ASIAN Percent of Charge...        0.000000                  0.075000\n",
      "78   INTERACTION: D_BLACK X BLACK Percent of Charge...        0.000000                  0.066000\n",
      "12                                  Commercial Vehicle        0.000000                  0.079000\n",
      "30                           SERO - (D_Violation Type)        0.000000                  0.000000\n",
      "15                             Contributed To Accident        0.000000                 -0.066000\n",
      "16                                             Car Age        0.000000                 -0.001000\n",
      "56                       GEO: ASIAN Racial Composition        0.000000                  0.021000\n",
      "14                                           Work Zone        0.000000                  0.027000\n",
      "59               GEO: WHITE Number of Speeding Charges       -0.000000                  0.000000\n",
      "28                                 S15 - (D_SubAgency)        0.000000                  0.000000\n",
      "62                       GEO: WHITE Racial Composition        0.000000                  0.021000\n",
      "11                                              HAZMAT        0.000000                 -0.031000\n",
      "29                                 W15 - (D_SubAgency)        0.000000                  0.000000\n",
      "63     GEO: WHITE Percent of Charges that were CHANGED        0.000000                  0.589000\n",
      "10                                     Property Damage        0.000000                 -0.049000\n",
      "54            GEO: ASIAN Average Speed NOT in 9,14 MPH       -0.000000                  0.037000\n",
      "65               GEO: TOTAL Number of Speeding Charges       -0.000000                  0.000000\n",
      "48               GEO: OTHER Number of Speeding Charges        0.000000                  0.000000\n",
      "53               GEO: ASIAN Number of Speeding Charges       -0.000000                  0.000000\n",
      "7                                             Accident        0.000000                 -0.071000\n",
      "42            GEO: HISPANIC Number of Speeding Charges        0.000000                  0.000000\n",
      "39                       GEO: BLACK Racial Composition        0.000000                  0.021000\n",
      "71     GEO: NATIVE AMERICAN Number of Speeding Charges       -0.000000                  0.000000\n",
      "36               GEO: BLACK Number of Speeding Charges        0.000000                  0.000000\n",
      "69     GEO: TOTAL Percent of Charges that were CHANGED       -0.000000                  0.589000\n",
      "9                                      Personal Injury        0.000000                 -0.001000\n",
      "34                                  Driver State != MD        0.000000                 -0.116000\n",
      "86   INTERACTION: D_BLACK X OTHER Average Speed NOT...       -0.001000                 -0.137000\n",
      "61        GEO: WHITE Number of Citations for Each Stop       -0.001000                  0.152000\n",
      "45                    GEO: HISPANIC Racial Composition       -0.002000                  0.021000\n",
      "132    INTERACTION: D_WHITE X OTHER Racial Composition       -0.002000                  0.105000\n",
      "94   INTERACTION: D_HISPANIC X BLACK Percent of Cha...       -0.003000                  0.029000\n",
      "99   INTERACTION: D_HISPANIC X HISPANIC Average Spe...       -0.005000                 -0.119000\n",
      "88   INTERACTION: D_BLACK X WHITE Percent of Charge...       -0.006000                  0.066000\n",
      "143  INTERACTION: D_ASIAN X BLACK Average Speed NOT...       -0.009000                 -0.108000\n",
      "83   INTERACTION: D_BLACK X HISPANIC Average Speed ...       -0.009000                 -0.137000\n",
      "127  INTERACTION: D_WHITE X BLACK Average Speed NOT...       -0.010000                 -0.165000\n",
      "153  INTERACTION: D_ASIAN X WHITE Average Speed NOT...       -0.010000                 -0.108000\n",
      "156  INTERACTION: D_ASIAN X ASIAN Average Speed NOT...       -0.011000                 -0.108000\n",
      "95   INTERACTION: D_HISPANIC X BLACK Average Speed ...       -0.012000                 -0.119000\n",
      "128                      INTERACTION: D_WHITE X D_Male       -0.012000                 -1.580000\n",
      "67        GEO: TOTAL Number of Citations for Each Stop       -0.013000                  0.152000\n",
      "35                                      DL State != MD       -0.013000                 -0.201000\n",
      "147  INTERACTION: D_ASIAN X HISPANIC Average Speed ...       -0.014000                 -0.108000\n",
      "68                       GEO: TOTAL Racial Composition       -0.016000                  0.021000\n",
      "134  INTERACTION: D_WHITE X OTHER Average Speed NOT...       -0.016000                 -0.165000\n",
      "108  INTERACTION: D_HISPANIC X ASIAN Average Speed ...       -0.016000                 -0.119000\n",
      "137  INTERACTION: D_WHITE X WHITE Average Speed NOT...       -0.017000                 -0.165000\n",
      "72   GEO: NATIVE AMERICAN Average Speed NOT in 9,14...       -0.017000                  0.037000\n",
      "126  INTERACTION: D_WHITE X BLACK Percent of Charge...       -0.019000                  0.315000\n",
      "6                                            Longitude       -0.019000                  0.255000\n",
      "89   INTERACTION: D_BLACK X WHITE Average Speed NOT...       -0.021000                 -0.137000\n",
      "130  INTERACTION: D_WHITE X HISPANIC Percent of Cha...       -0.021000                  0.315000\n",
      "66            GEO: TOTAL Average Speed NOT in 9,14 MPH       -0.024000                  0.037000\n",
      "55        GEO: ASIAN Number of Citations for Each Stop       -0.027000                  0.152000\n",
      "79   INTERACTION: D_BLACK X BLACK Average Speed NOT...       -0.029000                 -0.137000\n",
      "131  INTERACTION: D_WHITE X HISPANIC Average Speed ...       -0.029000                 -0.165000\n",
      "38        GEO: BLACK Number of Citations for Each Stop       -0.030000                  0.152000\n",
      "85   INTERACTION: D_BLACK X OTHER Percent of Charge...       -0.035000                  0.066000\n",
      "91   INTERACTION: D_BLACK X ASIAN Percent of Charge...       -0.049000                  0.066000\n",
      "136  INTERACTION: D_WHITE X WHITE Percent of Charge...       -0.051000                  0.315000\n",
      "73   GEO: NATIVE AMERICAN Number of Citations for E...       -0.052000                  0.152000\n",
      "98   INTERACTION: D_HISPANIC X HISPANIC Percent of ...       -0.053000                  0.029000\n",
      "107  INTERACTION: D_HISPANIC X ASIAN Percent of Cha...       -0.054000                  0.029000\n",
      "3                                        Driving Speed       -0.059000                 -1.029000\n",
      "96                    INTERACTION: D_HISPANIC X D_Male       -0.102000                  0.361000\n",
      "57     GEO: ASIAN Percent of Charges that were CHANGED       -0.103000                  0.589000\n",
      "51     GEO: OTHER Percent of Charges that were CHANGED       -0.121000                  0.589000\n",
      "80                       INTERACTION: D_BLACK X D_Male       -0.132000                  0.012000\n",
      "50        GEO: OTHER Number of Citations for Each Stop       -0.134000                  0.152000\n",
      "40     GEO: BLACK Percent of Charges that were CHANGED       -0.223000                  0.589000\n",
      "33                                                Male       -0.229000                  0.035000\n",
      "76   GEO: NATIVE AMERICAN Percent of Charges that w...       -0.232000                 -0.738000\n",
      "41   GEO: BLACK Percent of Charges that were NOT CH...       -0.246000                 -0.738000\n",
      "104  INTERACTION: D_HISPANIC X WHITE Percent of Cha...       -0.250000                  0.029000\n",
      "46   GEO: HISPANIC Percent of Charges that were CHA...       -0.273000                  0.589000\n",
      "13                                             Alcohol       -0.353000                 -2.010000\n",
      "1                                  Number of Citations       -0.383000                 -0.626000\n",
      "47   GEO: HISPANIC Percent of Charges that were NOT...       -0.459000                 -0.738000\n",
      "70   GEO: TOTAL Percent of Charges that were NOT CH...       -0.548000                 -0.738000\n",
      "64   GEO: WHITE Percent of Charges that were NOT CH...       -0.643000                 -0.738000\n",
      "31                           No - (D_Search Conducted)       -1.143000                  0.199000\n",
      "32                          Yes - (D_Search Conducted)       -2.024000                 -2.967000\n",
      "58   GEO: ASIAN Percent of Charges that were NOT CH...       -2.514000                 -0.738000\n",
      "4                              Speed Over Posted Limit       -2.613000                 -2.107000\n",
      "52   GEO: OTHER Percent of Charges that were NOT CH...       -2.790000                 -0.738000\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 0):\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Accuracy\n",
    "\n",
    "~~**NOTE: THIS WAS NOT SCORING 100% UNTIL ADDING RACExGEO, RACExMALE INTERACTION TERMS; and changing \"avg speed over limit\" to \"avg speed over limit not in 9,14 mph\"**~~\n",
    "\n",
    "~~(it had ~98% accuracy)~~\n",
    "\n",
    "Now back to scoring ~98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Stops that appeared to receive leniency == 92.914% \n",
      "\n",
      "\n",
      "l1_model TRAINING accuracy == 98.318754702784%\n",
      "\n",
      "l1_model TESTING accuracy == 98.316% \n",
      " \n",
      "\n",
      "log_model TRAINING accuracy == 97.844%\n",
      "\n",
      "log_model TESTING accuracy == 97.846%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percent of Stops that appeared to receive leniency == { round(100 * np.mean(df[\"Speed Altered\"]), 3) }% \\n\\n')\n",
    "\n",
    "print(f'l1_model TRAINING accuracy == {round(100 * l1_model.score(X_train, y_train), 13)}%\\n')\n",
    "\n",
    "print(f'l1_model TESTING accuracy == {round(100 * l1_model.score(X_test, y_test), 3)}% \\n \\n')\n",
    "\n",
    "\n",
    "print(f'log_model TRAINING accuracy == {round(100 * log_model.score(X_train, y_train), 3)}%\\n')\n",
    "\n",
    "print(f'log_model TESTING accuracy == {round(100 * log_model.score(X_test, y_test), 3)}%')\n",
    "\n",
    "\n",
    "# print(f'OLS_model TRAINING accuracy == {round(100 * OLS_model.score(X_train, y_train), 3)}%\\n')\n",
    "\n",
    "# print(f'OLS_model TESTING accuracy == {round(100 * OLS_model.score(X_test, y_test), 3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1 Coefficient</th>\n",
       "      <th>Normal Logit Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L1 Coefficient</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.93199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal Logit Coefficient</th>\n",
       "      <td>0.93199</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          L1 Coefficient  Normal Logit Coefficient\n",
       "L1 Coefficient                   1.00000                   0.93199\n",
       "Normal Logit Coefficient         0.93199                   1.00000"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Intercept\n",
    "\n",
    "Can we run this without an intercept? The intercept term is much larger than the next biggest coefficient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model_no_intercept = LogisticRegression(penalty='l1', solver='liblinear', fit_intercept=False)\n",
    "l1_model_no_intercept.fit(X, y)\n",
    "\n",
    "l1_coefficients_no_intercept = l1_model_no_intercept.coef_.tolist()[0]\n",
    "\n",
    "zipped_l1_coefs = list(zip(list(regression_df.columns), [round(x,3) for x in l1_coefficients_no_intercept]))\n",
    "\n",
    "log_model_no_intercept = LogisticRegression(max_iter=5000, fit_intercept=False) #default max_iter==100\n",
    "log_model_no_intercept.fit(X, y)\n",
    "\n",
    "log_coefficients_no_int = log_model_no_intercept.coef_.tolist()[0]\n",
    "\n",
    "zipped_log_coefs_no_intercept = list(zip(list(regression_df.columns), [round(x,3) for x in log_coefficients_no_int]))\n",
    "\n",
    "no_intercept_results_df = pd.DataFrame({'Variable': [x[0] for x in zipped_l1_coefs],\n",
    "                           'L1 Coefficient': [x[1] for x in zipped_l1_coefs],\n",
    "                           'Normal Logit Coefficient': [x[1] for x in zipped_log_coefs_no_intercept]})\n",
    "\n",
    "no_intercept_results_df = no_intercept_results_df.sort_values(by=['L1 Coefficient'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 0):\n",
    "    print(no_intercept_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percent of Stops that appeared to receive leniency == { round(100 * np.mean(df[\"Speed Altered\"]), 3) }% \\n\\n')\n",
    "\n",
    "print(f'l1_model_no_intercept TRAINING accuracy == {round(100 * l1_model_no_intercept.score(X_train, y_train), 3)}%\\n')\n",
    "\n",
    "print(f'l1_model_no_intercept TESTING accuracy == {round(100 * l1_model_no_intercept.score(X_test, y_test), 3)}% \\n \\n')\n",
    "\n",
    "\n",
    "print(f'log_model_no_intercept TRAINING accuracy == {round(100 * log_model_no_intercept.score(X_train, y_train), 3)}%\\n')\n",
    "\n",
    "print(f'log_model_no_intercept TESTING accuracy == {round(100 * log_model_no_intercept.score(X_test, y_test), 3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Table\n",
    "\n",
    "Much lower correlation than with an intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_intercept_results_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test that each race has the same proclivity to speed (% speeds >14 mph)\n",
    "\n",
    "An implicit assumption is that all races have the same speeding distribution - at least within the ranges of 9-14 MPH. \n",
    "\n",
    "\\\n",
    "However, this is not necessarily the case:\n",
    "\\\n",
    "\n",
    "    As the following cells show, most races seem to follow their own unique speeding distribution (for speeding above 14 MPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLACK speed over 14 MPH == 21.268611232041792\n",
      "Average HISPANIC speed over 14 MPH == 21.342176039119803\n",
      "Average OTHER speed over 14 MPH == 20.889993972272453\n",
      "Average NATIVE AMERICAN speed over 14 MPH == 21.46511627906977\n",
      "Average WHITE speed over 14 MPH == 20.739335386913098\n",
      "Average ASIAN speed over 14 MPH == 20.468230403800476\n"
     ]
    }
   ],
   "source": [
    "def avg_speeds_above_14(race):\n",
    "    temp_list = []\n",
    "    for x in zip(df[f\"{race} - (D_Race)\"], df['Speed Over Posted Limit']):\n",
    "        if x[0]==1 and x[1]>14:\n",
    "            temp_list.append(x[1])\n",
    "            \n",
    "    return np.mean(temp_list)\n",
    "\n",
    "for race in race_set:\n",
    "    print(f\"Average {race} speed over 14 MPH == {avg_speeds_above_14(race)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Z test for different means (speeding above 14 mph), with hypothesized difference between means==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_above_14(race1, race2):\n",
    "    x1 = np.array([x[0] for x in zip(df['Speed Over Posted Limit'], df['Race']) if x[1]==race1 and 14 < x[0] < 30])\n",
    "    x2 = np.array([x[0] for x in zip(df['Speed Over Posted Limit'], df[\"Race\"]) if x[1]==race2 and 14 < x[0] < 30])\n",
    "\n",
    "    xbar1 = np.mean(x1)\n",
    "    xbar2 = np.mean(x2)\n",
    "    \n",
    "    sig1 = np.std(x1)\n",
    "    sig2 = np.std(x2)\n",
    "    \n",
    "    n1 = len(x1)\n",
    "    n2 = len(x2)\n",
    "    \n",
    "    z = (xbar1-xbar2)/np.sqrt((sig1**2)/n1 + (sig2**2)/n2)\n",
    "    \n",
    "    return abs(round(z,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col1 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col0 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col2 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col4 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col5 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col1 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col1 {\n",
       "            background:  red;\n",
       "        }    #T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col1 {\n",
       "            background:  red;\n",
       "        }</style><table id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64b\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ASIAN</th>        <th class=\"col_heading level0 col1\" >BLACK</th>        <th class=\"col_heading level0 col2\" >HISPANIC</th>        <th class=\"col_heading level0 col3\" >NATIVE AMERICAN</th>        <th class=\"col_heading level0 col4\" >OTHER</th>        <th class=\"col_heading level0 col5\" >WHITE</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row0\" class=\"row_heading level0 row0\" >ASIAN</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col1\" class=\"data row0 col1\" >2.660000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col2\" class=\"data row0 col2\" >0.260000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col3\" class=\"data row0 col3\" >1.020000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col4\" class=\"data row0 col4\" >0.550000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow0_col5\" class=\"data row0 col5\" >1.050000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row1\" class=\"row_heading level0 row1\" >BLACK</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col0\" class=\"data row1 col0\" >2.660000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col2\" class=\"data row1 col2\" >3.190000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col3\" class=\"data row1 col3\" >0.710000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col4\" class=\"data row1 col4\" >3.250000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow1_col5\" class=\"data row1 col5\" >2.850000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row2\" class=\"row_heading level0 row2\" >HISPANIC</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col0\" class=\"data row2 col0\" >0.260000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col1\" class=\"data row2 col1\" >3.190000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col3\" class=\"data row2 col3\" >0.990000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col4\" class=\"data row2 col4\" >0.900000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow2_col5\" class=\"data row2 col5\" >1.050000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row3\" class=\"row_heading level0 row3\" >NATIVE AMERICAN</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col0\" class=\"data row3 col0\" >1.020000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col1\" class=\"data row3 col1\" >0.710000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col2\" class=\"data row3 col2\" >0.990000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col4\" class=\"data row3 col4\" >1.110000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow3_col5\" class=\"data row3 col5\" >0.910000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row4\" class=\"row_heading level0 row4\" >OTHER</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col0\" class=\"data row4 col0\" >0.550000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col1\" class=\"data row4 col1\" >3.250000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col2\" class=\"data row4 col2\" >0.900000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col3\" class=\"data row4 col3\" >1.110000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow4_col5\" class=\"data row4 col5\" >1.720000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64blevel0_row5\" class=\"row_heading level0 row5\" >WHITE</th>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col0\" class=\"data row5 col0\" >1.050000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col1\" class=\"data row5 col1\" >2.850000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col2\" class=\"data row5 col2\" >1.050000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col3\" class=\"data row5 col3\" >0.910000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col4\" class=\"data row5 col4\" >1.720000</td>\n",
       "                        <td id=\"T_b8e5c6c0_e957_11ea_927f_7c2a31fda64brow5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2982724df08>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_tests_above_14_table = pd.DataFrame({race1:[z_score_above_14(race1, race2) for race2 in sorted(list(race_set))] for race1 in sorted(list(race_set))}, index=sorted(list(race_set)))\n",
    "\n",
    "\n",
    "z_tests_above_14_table.style.apply(lambda x: [\"background: red\" if v > 1.96 else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Races probably have different speeding distributions\n",
    "\n",
    "**We should run a test to specifically test similarity of distributions**\n",
    "\n",
    "**Utlimately, we need to investigate whether white/asian/etc. drivers *appear* to receive more leniency because they have more stops actually at 9 mph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "237.997px",
    "width": "221.449px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
